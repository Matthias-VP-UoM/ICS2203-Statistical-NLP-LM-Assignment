{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab339560",
   "metadata": {},
   "source": [
    "# ICS2203 - Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101298d-721f-46f7-bed4-b75067e060b4",
   "metadata": {},
   "source": [
    "Link to download created models:\n",
    "https://drive.google.com/file/d/1ean20gz4DwEgM9Yr4zuvjpKhnkxkuiZP/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209305d2",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71569ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # to access the corpus files\n",
    "import psutil  # to check memory usage after building language models\n",
    "import time  # to check how long it takes to extract corpus and build frequency counts\n",
    "import math  # to calculate perplexity\n",
    "import pickle  # to save the corpus and all models\n",
    "import re  # to remove punctuation from sentences\n",
    "import pandas as pd  # to convert perplexity table from 2D list to Dataframe\n",
    "import numpy as np  # to handle values out of range for math.exp() and to generate sentence\n",
    "import xml.etree.ElementTree as ET  # to extract the text from the appropriate tags in the corpus files\n",
    "import operator  # to accompany the reduce function below\n",
    "from functools import reduce  # to multiply probabilities in a shorthand way\n",
    "from collections import Counter  # to tally frequency counts\n",
    "from sklearn.model_selection import train_test_split  # to split the extracted corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db5944-e32b-4a8b-8dfc-22e792a6eb6b",
   "metadata": {},
   "source": [
    "## Setting up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2577b5e3-439d-4cfa-994e-99849f109cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a main directory for the corpus and models for organization purposes\n",
    "models_sub = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206cb67e-5ded-4304-a4a7-f788b622c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_sen_filename = models_sub+\"/train_corpus_sen.pkl\"\n",
    "test_corpus_sen_filename = models_sub+\"/test_corpus_sen.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1715102-a2ec-4846-83d7-589daf39f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting subdirectories for the models in order to be organized\n",
    "vanilla_dict_filename = models_sub+\"/vanilla/vanilla_uni_model.pkl\"\n",
    "vanilla_dict_bi_filename = models_sub+\"/vanilla/vanilla_bi_model.pkl\"\n",
    "vanilla_dict_tri_filename = models_sub+\"/vanilla/vanilla_tri_model.pkl\"\n",
    "laplace_dict_filename = models_sub+\"/laplace/laplace_uni_model.pkl\"\n",
    "laplace_dict_bi_filename = models_sub+\"/laplace/laplace_bi_model.pkl\"\n",
    "laplace_dict_tri_filename = models_sub+\"/laplace/laplace_tri_model.pkl\"\n",
    "unk_dict_filename = models_sub+\"/unk/unk_uni_model.pkl\"\n",
    "unk_dict_bi_filename = models_sub+\"/unk/unk_bi_model.pkl\"\n",
    "unk_dict_tri_filename = models_sub+\"/unk/unk_tri_model.pkl\"\n",
    "unk_laplace_dict_filename = models_sub+\"/unk_laplace/unk_laplace_uni_model.pkl\"\n",
    "unk_laplace_dict_bi_filename = models_sub+\"/unk_laplace/unk_laplace_bi_model.pkl\"\n",
    "unk_laplace_dict_tri_filename = models_sub+\"/unk_laplace/unk_laplace_tri_model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca1318-0251-480a-b620-d7318b6f7048",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edf149",
   "metadata": {},
   "source": [
    "### a) Importing corpus files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833ce5a-ff3c-4006-9221-f9f3396f4164",
   "metadata": {},
   "source": [
    "#### The corpus has an overall size of 185MB in storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892a935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'corpus_eng/download/Texts'\n",
    "corpus_sen = []\n",
    "corpus_sen_cat = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5ebcc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1E.xml',\n",
       " 'A1F.xml',\n",
       " 'A1G.xml',\n",
       " 'A1H.xml',\n",
       " 'A1J.xml',\n",
       " 'A1K.xml',\n",
       " 'A1L.xml',\n",
       " 'A1M.xml',\n",
       " 'A1N.xml',\n",
       " 'A1P.xml',\n",
       " 'A1U.xml',\n",
       " 'A1X.xml',\n",
       " 'A2D.xml',\n",
       " 'A31.xml',\n",
       " 'A36.xml',\n",
       " 'A38.xml',\n",
       " 'A39.xml',\n",
       " 'A3C.xml',\n",
       " 'A3E.xml',\n",
       " 'A3K.xml',\n",
       " 'A3M.xml',\n",
       " 'A3P.xml',\n",
       " 'A4D.xml',\n",
       " 'A5E.xml',\n",
       " 'A7S.xml',\n",
       " 'A7T.xml',\n",
       " 'A7W.xml',\n",
       " 'A7X.xml',\n",
       " 'A7Y.xml',\n",
       " 'A80.xml',\n",
       " 'A82.xml',\n",
       " 'A84.xml',\n",
       " 'A8L.xml',\n",
       " 'A8M.xml',\n",
       " 'A8N.xml',\n",
       " 'A8P.xml',\n",
       " 'A8R.xml',\n",
       " 'A8S.xml',\n",
       " 'A8T.xml',\n",
       " 'A8U.xml',\n",
       " 'A91.xml',\n",
       " 'A97.xml',\n",
       " 'A98.xml',\n",
       " 'A9G.xml',\n",
       " 'A9J.xml',\n",
       " 'A9P.xml',\n",
       " 'A9X.xml',\n",
       " 'A9Y.xml',\n",
       " 'AA3.xml',\n",
       " 'AA6.xml',\n",
       " 'AAM.xml',\n",
       " 'AAR.xml',\n",
       " 'AHB.xml',\n",
       " 'AHC.xml',\n",
       " 'AHD.xml',\n",
       " 'AHE.xml',\n",
       " 'AHF.xml',\n",
       " 'AHH.xml',\n",
       " 'AHL.xml',\n",
       " 'AJ1.xml',\n",
       " 'AJF.xml',\n",
       " 'AJG.xml',\n",
       " 'AJW.xml',\n",
       " 'AL0.xml',\n",
       " 'AL2.xml',\n",
       " 'AL5.xml',\n",
       " 'BM4.xml',\n",
       " 'CBD.xml',\n",
       " 'CBE.xml',\n",
       " 'CBM.xml',\n",
       " 'CEL.xml',\n",
       " 'CFC.xml',\n",
       " 'CH3.xml',\n",
       " 'E9S.xml',\n",
       " 'K29.xml',\n",
       " 'K2A.xml',\n",
       " 'K2B.xml',\n",
       " 'K2C.xml',\n",
       " 'K2E.xml',\n",
       " 'K2N.xml',\n",
       " 'K36.xml',\n",
       " 'K37.xml',\n",
       " 'K38.xml',\n",
       " 'K39.xml',\n",
       " 'K3A.xml',\n",
       " 'K3B.xml',\n",
       " 'K3C.xml',\n",
       " 'K3D.xml',\n",
       " 'K4R.xml',\n",
       " 'K4S.xml',\n",
       " 'K4U.xml',\n",
       " 'K4Y.xml',\n",
       " 'K58.xml',\n",
       " 'K5B.xml',\n",
       " 'K5C.xml',\n",
       " 'K5E.xml',\n",
       " 'K5K.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_docs = [f for f in os.listdir(corpus_path+'/news')]\n",
    "\n",
    "# Printing the contents for testing purposes\n",
    "news_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b627b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A6U.xml',\n",
       " 'ACJ.xml',\n",
       " 'ALP.xml',\n",
       " 'AMM.xml',\n",
       " 'AS6.xml',\n",
       " 'B17.xml',\n",
       " 'B1G.xml',\n",
       " 'B2K.xml',\n",
       " 'CLP.xml',\n",
       " 'CLW.xml',\n",
       " 'CMA.xml',\n",
       " 'CRS.xml',\n",
       " 'CTY.xml',\n",
       " 'EA7.xml',\n",
       " 'ECV.xml',\n",
       " 'EW1.xml',\n",
       " 'EWW.xml',\n",
       " 'F98.xml',\n",
       " 'F9V.xml',\n",
       " 'FC1.xml',\n",
       " 'FEF.xml',\n",
       " 'FPG.xml',\n",
       " 'FSS.xml',\n",
       " 'FT1.xml',\n",
       " 'HRG.xml',\n",
       " 'HWV.xml',\n",
       " 'HXH.xml',\n",
       " 'J18.xml',\n",
       " 'J57.xml',\n",
       " 'J7G.xml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aca_docs = [f for f in os.listdir(corpus_path+'/aca')]\n",
    "\n",
    "# Printing the contents for testing purposes\n",
    "aca_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c7fd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KB5.xml',\n",
       " 'KB7.xml',\n",
       " 'KBC.xml',\n",
       " 'KBD.xml',\n",
       " 'KBH.xml',\n",
       " 'KBJ.xml',\n",
       " 'KBP.xml',\n",
       " 'KBW.xml',\n",
       " 'KCC.xml',\n",
       " 'KCF.xml',\n",
       " 'KCU.xml',\n",
       " 'KCV.xml',\n",
       " 'KD0.xml',\n",
       " 'KD1.xml',\n",
       " 'KD3.xml',\n",
       " 'KD7.xml',\n",
       " 'KD8.xml',\n",
       " 'KDD.xml',\n",
       " 'KDF.xml',\n",
       " 'KDJ.xml',\n",
       " 'KE2.xml',\n",
       " 'KE4.xml',\n",
       " 'KNR.xml',\n",
       " 'KP2.xml',\n",
       " 'KP5.xml',\n",
       " 'KP7.xml',\n",
       " 'KPU.xml',\n",
       " 'KPX.xml',\n",
       " 'KSN.xml',\n",
       " 'KSW.xml']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_docs = [f for f in os.listdir(corpus_path+'/dem')]\n",
    "\n",
    "# Printing the contents for testing purposes\n",
    "dem_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981e3905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AB9.xml',\n",
       " 'AC2.xml',\n",
       " 'BMW.xml',\n",
       " 'BPA.xml',\n",
       " 'C8T.xml',\n",
       " 'CB5.xml',\n",
       " 'CCW.xml',\n",
       " 'CDB.xml',\n",
       " 'CFY.xml',\n",
       " 'FAJ.xml',\n",
       " 'FET.xml',\n",
       " 'FPB.xml',\n",
       " 'G01.xml',\n",
       " 'G0L.xml',\n",
       " 'G0S.xml',\n",
       " 'G0Y.xml',\n",
       " 'GUU.xml',\n",
       " 'GVL.xml',\n",
       " 'H85.xml',\n",
       " 'H9C.xml',\n",
       " 'H9D.xml',\n",
       " 'HR9.xml',\n",
       " 'J10.xml',\n",
       " 'J54.xml',\n",
       " 'K8V.xml']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fic_docs = [f for f in os.listdir(corpus_path+'/fic')]\n",
    "\n",
    "# Printing the contents for testing purposes\n",
    "fic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a59ddd",
   "metadata": {},
   "source": [
    "### b) Extracting corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36bbf6e6-a7ac-438e-b618-81bf5c8d5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s_tags_wtext(xml_file, cat):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find the <wtext> tag\n",
    "    wtext_tag = root.find(\".//wtext[@type='\"+str(cat)+\"']\")\n",
    "    if wtext_tag is not None:\n",
    "        # Find all <s> tags within <wtext>\n",
    "        s_tags = wtext_tag.findall('.//s')\n",
    "        \n",
    "        # Extract the <s> tags along with the actual tags\n",
    "        result = []\n",
    "        for s_tag in s_tags:\n",
    "            s_with_text = f'<{s_tag.tag}> '  # Start with the opening <s> tag\n",
    "            \n",
    "            # Include direct text and tail of each child element within <s> tag\n",
    "            for child in s_tag:\n",
    "                if child.text:\n",
    "                    s_with_text += child.text.lower()\n",
    "                if child.tail:\n",
    "                    s_with_text += child.tail.lower()\n",
    "            \n",
    "            s_with_text += f' </{s_tag.tag}>'  # Add the closing </s> tag\n",
    "            result.append(s_with_text)\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6e9d3c-056a-4213-ad08-df2b921a5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s_tags_stext(xml_file, cat):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Find the <wtext> tag\n",
    "    wtext_tag = root.find(\".//stext[@type='\"+str(cat)+\"']\")\n",
    "    if wtext_tag is not None:\n",
    "        # Find all <s> tags within <wtext>\n",
    "        s_tags = wtext_tag.findall('.//s')\n",
    "        \n",
    "        # Extract the <s> tags along with the actual tags\n",
    "        result = []\n",
    "        for s_tag in s_tags:\n",
    "            s_with_text = f'<{s_tag.tag}> '  # Start with the opening <s> tag\n",
    "            \n",
    "            # Include direct text and tail of each child element within <s> tag\n",
    "            for child in s_tag:\n",
    "                if child.text:\n",
    "                    s_with_text += child.text.lower()\n",
    "                if child.tail:\n",
    "                    s_with_text += child.tail.lower()\n",
    "            \n",
    "            s_with_text += f' </{s_tag.tag}>'  # Add the closing </s> tag\n",
    "            result.append(s_with_text)\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6379bf-780d-4723-8fa4-591c9be2eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation marks from a sentence\n",
    "def remove_punctuation(sen):\n",
    "    result = re.sub(r'[^\\w\\s]', '', sen)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f814fa85-12c6-4249-96f8-238c1d7314ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the news files in the corpus directory\n",
    "corpus_sen_news = []\n",
    "news_start_time = time.time()\n",
    "for news_doc in news_docs:\n",
    "    file_path = corpus_path+'/news/'+str(news_doc)\n",
    "    if (extract_s_tags_wtext(file_path, 'NEWS') is None):\n",
    "        text = extract_s_tags_stext(file_path, 'NEWS')\n",
    "    else:\n",
    "        text = extract_s_tags_wtext(file_path, 'NEWS')\n",
    "    corpus_sen_news.append(text)\n",
    "    corpus_sen.append(text)\n",
    "\n",
    "news_end_time = time.time()\n",
    "news_time = news_end_time - news_start_time\n",
    "corpus_sen_cat['news'] = corpus_sen_news.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b0f892e-2ea5-41c6-9be6-b55df2efdf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the fiction files in the corpus directory\n",
    "corpus_sen_fic = []\n",
    "fic_start_time = time.time()\n",
    "for fic_doc in fic_docs:\n",
    "    file_path = corpus_path+'/fic/'+str(fic_doc)\n",
    "    if (extract_s_tags_wtext(file_path, 'FICTION') is None):\n",
    "        text = extract_s_tags_stext(file_path, 'FICTION')\n",
    "    else:\n",
    "        text = extract_s_tags_wtext(file_path, 'FICTION')\n",
    "    corpus_sen_fic.append(text)\n",
    "    corpus_sen.append(text)\n",
    "\n",
    "fic_end_time = time.time()\n",
    "fic_time = fic_end_time - fic_start_time\n",
    "corpus_sen_cat['fic'] = corpus_sen_fic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd2d846c-ab5d-416b-adb5-ac0c18861488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dem files in the corpus directory\n",
    "corpus_sen_dem = []\n",
    "dem_start_time = time.time()\n",
    "for dem_doc in dem_docs:\n",
    "    file_path = corpus_path+'/dem/'+str(dem_doc)\n",
    "    if (extract_s_tags_wtext(file_path, 'CONVRSN') is None):\n",
    "        text = extract_s_tags_stext(file_path, 'CONVRSN')\n",
    "    else:\n",
    "        text = extract_s_tags_wtext(file_path, 'CONVRSN')\n",
    "    corpus_sen_dem.append(text)\n",
    "    corpus_sen.append(text)\n",
    "\n",
    "dem_end_time = time.time()\n",
    "dem_time = dem_end_time - dem_start_time\n",
    "corpus_sen_cat['dem'] = corpus_sen_dem.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da59bba3-b5b5-459e-b030-8add22d4060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the aca files in the corpus directory\n",
    "corpus_sen_aca = []\n",
    "aca_start_time = time.time()\n",
    "for aca_doc in aca_docs:\n",
    "    file_path = corpus_path+'/aca/'+str(aca_doc)\n",
    "    if (extract_s_tags_wtext(file_path, 'ACPROSE') is None):\n",
    "        text = extract_s_tags_stext(file_path, 'ACPROSE')\n",
    "    else:\n",
    "        text = extract_s_tags_wtext(file_path, 'ACPROSE')\n",
    "    \n",
    "    if text is None:\n",
    "        text = extract_s_tags_wtext(file_path, 'UNPUB')\n",
    "    corpus_sen_aca.append(text)\n",
    "    corpus_sen.append(text)\n",
    "\n",
    "aca_end_time = time.time()\n",
    "aca_time = aca_end_time - aca_start_time\n",
    "corpus_sen_cat['aca'] = corpus_sen_aca.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27343bc-710f-4a22-9a48-8239bd4a644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract the corpus: 29.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to extract the corpus\n",
    "total_extract_time = news_time + fic_time + dem_time + aca_time\n",
    "print(f\"Time taken to extract the corpus: {round(total_extract_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6598d3-843d-4a3d-b4da-64284d419a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the corpus and remove any punctuation marks from the sentences\n",
    "for i in range(len(corpus_sen)):\n",
    "    for j, sentence in enumerate(corpus_sen[i]):\n",
    "        text_words = sentence.split()\n",
    "        text_words_string = ' '.join(text_words[1:-1])\n",
    "        cleaned_text = \"<s> \" + remove_punctuation(text_words_string) + \" </s>\"\n",
    "        corpus_sen[i][j] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92b3e3c6-4d22-4c14-a4ee-bc2adb0eb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_sen_news[10] (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524d4032-ada0-4954-be36-6662fe7de46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#corpus_sen[169] (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48518c82-a1d2-4b70-9949-0128a09aaa58",
   "metadata": {},
   "source": [
    "### Splitting corpus into training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec53d9-0afa-42af-9a42-093ab31ea2ca",
   "metadata": {},
   "source": [
    "#### Here the corpus is split with a ratio of [4:1]. This means that 80% of the corpus is used as the training set, while the remaining 20% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1eac0ca-70ea-4357-85c7-91553deae6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus, filename): \n",
    "    with open(filename, 'wb') as file: \n",
    "        pickle.dump(corpus, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "105cddcc-19d4-4fa4-bd9f-3e5d2986e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(corpus):\n",
    "    train_corpus, test_corpus = train_test_split(corpus, test_size=0.2, random_state=42)\n",
    "\n",
    "    return train_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3f46134-4bee-46a5-8c31-693c7a62aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_sen, test_corpus_sen = split_data(corpus_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6865a3f-9634-4a26-8f1f-eef138ed9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_corpus(train_corpus_sen, train_corpus_sen_filename)\n",
    "save_corpus(test_corpus_sen, test_corpus_sen_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f80b6217-b732-4197-8cb9-5530fe968dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['laplace',\n",
       " 'test_corpus_sen.pkl',\n",
       " 'train_corpus_sen.pkl',\n",
       " 'unk',\n",
       " 'unk_laplace',\n",
       " 'vanilla']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d77a8ff-d369-41ee-b81a-7ff1b83eb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_corpus_sen[0:5][0:10] (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e18820-f2ce-4799-9b8d-d17d6b1f44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_corpus_sen[0:10][0:20] (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38049f4-2615-4d42-992a-fd18416586b6",
   "metadata": {},
   "source": [
    "### Loading training and test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "325f1039-cd81-463e-8aae-1b7ac64b0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        corpus_to_get = pickle.load(file) \n",
    "\n",
    "    return corpus_to_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b229159-7060-4296-8075-2ccda2f1ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_sen = load_corpus(train_corpus_sen_filename)\n",
    "test_corpus_sen = load_corpus(test_corpus_sen_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4b0ea36-dad7-401d-818f-87f4e4937c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus_sen[0:5][0:10] (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b045bbc-8c1d-4d7e-8b90-c306ffbea426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_corpus_sen[0:10][0:20] (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "581624c1-8f4a-48a0-8739-52e17aa67573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f7b04-6a9a-45bc-8fb9-03e032dfc80e",
   "metadata": {},
   "source": [
    "### c) Building Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a63af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating constant values for specific n-grams\n",
    "UNIGRAM_VAL = 1\n",
    "BIGRAM_VAL = 2\n",
    "TRIGRAM_VAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71003fcf-2bac-48c7-add8-ca70aaa7bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_gram_freq_counts(corpus, n):\n",
    "    count_dict = Counter()\n",
    "\n",
    "    # Calculate the frequency of each n-gram in every document\n",
    "    for text in corpus:\n",
    "        for sen in text:\n",
    "            words = sen.split()\n",
    "            #print(words)\n",
    "            if n == 1:\n",
    "                count_dict.update(words)\n",
    "            elif n > 1:\n",
    "                for i in range(len(words) - n + 1):\n",
    "                    ngram = ' '.join(words[i:i+n])\n",
    "                    count_dict[ngram] += 1\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15da739",
   "metadata": {},
   "source": [
    "### i) Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3602b88-94da-4006-8caf-9eac8a134d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 272094, 'struggling': 73, 'dentists': 17, 'pull': 163, 'more': 5769, 'teeth': 204, '</s>': 272094, 'by': 13562, 'david': 544, 'fletcher': 20, 'health': 660, 'services': 455, 'correspondent': 101, 'are': 14997, 'pulling': 107, 'out': 6091, 'patients': 628, 'unnecessarily': 6, 'as': 15481, 'they': 14490, 'struggle': 92, 'to': 74111, 'maintain': 91, 'living': 545, 'standards': 149, 'under': 1442, 'a': 70175, 'new': 2966, 'government': 1084, 'contract': 198, 'survey': 183, 'showed': 339, 'yesterday': 1253, 'two': 6509, 'three': 3423, 'said': 10248, 'had': 14930, 'extracted': 23, 'that': 35411, 'might': 2391, 'have': 17787, 'filled': 137, 'before': 2824, 'the': 169625, 'was': 28451, 'introduced': 198, 'than': 3166, 'half': 1416, 'their': 6601, 'pay': 675, 'last': 2991, 'year': 1910, 'not': 15055, 'increased': 306, 'or': 10358, 'even': 2194, 'fell': 378, 'of': 76795, '866': 3, 'in': 53568, 'dental': 13, 'journal': 29, 'probe': 39, 'says': 1296, 'when': 7291, 'were': 9753, 'asked': 1108, 'whether': 1002, 'referred': 135, 'for': 24428, 'extraction': 22, 'any': 3615, 'which': 9695, 'done': 1647, 'introduction': 149, '1990': 171, '61': 51, 'while': 1713, 'nearly': 501, '37': 47, 'mr': 1982, 'jeremy': 28, 'cowan': 4, 'editor': 65, 'contracts': 46, 'practical': 154, 'effect': 587, 'reduce': 188, 'preventive': 29, 'dentistry': 1, 'extractions': 1, 'conclusions': 68, 'challenged': 36, 'joe': 284, 'rich': 176, 'chairman': 309, 'british': 895, 'associations': 49, 'general': 895, 'committee': 297}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "vanilla_uni_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the word and its frequency count\n",
    "train_vanilla_dict_sen = dict(build_n_gram_freq_counts(train_corpus_sen, UNIGRAM_VAL))\n",
    "\n",
    "# Variable to represent end time\n",
    "vanilla_uni_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "vanilla_uni_time = vanilla_uni_end_time - vanilla_uni_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_vanilla_dict_sen.items())[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2078ec0",
   "metadata": {},
   "source": [
    "### ii) Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "048e1f30-ff84-46d9-93ab-63fd7521c192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling': 4, 'struggling dentists': 1, 'dentists pull': 1, 'pull more': 1, 'more teeth': 4, 'teeth </s>': 58, '<s> by': 1090, 'by david': 54, 'david fletcher': 1, 'fletcher health': 1, 'health services': 31, 'services correspondent': 12, 'correspondent </s>': 89, '<s> dentists': 2, 'dentists are': 1, 'are pulling': 3, 'pulling out': 10, 'out patients': 1, 'patients teeth': 1, 'teeth unnecessarily': 1, 'unnecessarily as': 1, 'as they': 399, 'they struggle': 1, 'struggle to': 12, 'to maintain': 54, 'maintain living': 1, 'living standards': 7, 'standards under': 1, 'under a': 97, 'a new': 695, 'new government': 6, 'government contract': 1, 'contract a': 3, 'a survey': 31, 'survey showed': 4, 'showed yesterday': 1, 'yesterday </s>': 342, '<s> two': 619, 'two three': 535, 'three dentists': 1, 'dentists said': 1, 'said they': 109, 'they had': 752, 'had extracted': 3, 'extracted teeth': 1, 'teeth that': 2, 'that they': 640, 'they might': 115, 'might have': 365, 'have filled': 1, 'filled before': 1, 'before the': 442, 'the contract': 21, 'contract was': 2, 'was introduced': 18, 'introduced </s>': 13, '<s> more': 201, 'more than': 602, 'than half': 37, 'half said': 2, 'said their': 8, 'their pay': 4, 'pay last': 1, 'last year': 377, 'year had': 2, 'had not': 318, 'not increased': 2, 'increased or': 1, 'or even': 143, 'even fell': 1, 'fell </s>': 17, '<s> the': 16590, 'the survey': 36, 'survey of': 24, 'of 866': 1, '866 dentists': 1, 'dentists in': 1, 'in the': 15042, 'the dental': 2, 'dental journal': 1, 'journal the': 1, 'the probe': 1, 'probe says': 1, 'says when': 8, 'when dentists': 1, 'dentists were': 3, 'were asked': 17, 'asked whether': 8, 'whether they': 71, 'extracted or': 1, 'or referred': 2, 'referred for': 2, 'for extraction': 1, 'extraction any': 1, 'any teeth': 1, 'teeth which': 2, 'which they': 209, 'might not': 104, 'not have': 254, 'have done': 202}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "vanilla_bi_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_vanilla_dict_bi_sen = dict(build_n_gram_freq_counts(train_corpus_sen, BIGRAM_VAL))\n",
    "\n",
    "# Variable to represent end time\n",
    "vanilla_bi_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "vanilla_bi_time = vanilla_bi_end_time - vanilla_bi_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_vanilla_dict_bi_sen.items())[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff75b9",
   "metadata": {},
   "source": [
    "### iii) Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a36e94b-36a1-4c08-827a-ec4c9a390bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling dentists': 1, 'struggling dentists pull': 1, 'dentists pull more': 1, 'pull more teeth': 1, 'more teeth </s>': 1, '<s> by david': 42, 'by david fletcher': 1, 'david fletcher health': 1, 'fletcher health services': 1, 'health services correspondent': 12, 'services correspondent </s>': 12, '<s> dentists are': 1, 'dentists are pulling': 1, 'are pulling out': 3, 'pulling out patients': 1, 'out patients teeth': 1, 'patients teeth unnecessarily': 1, 'teeth unnecessarily as': 1, 'unnecessarily as they': 1, 'as they struggle': 1, 'they struggle to': 1, 'struggle to maintain': 1, 'to maintain living': 1, 'maintain living standards': 1, 'living standards under': 1, 'standards under a': 1, 'under a new': 5, 'a new government': 3, 'new government contract': 1, 'government contract a': 1, 'contract a survey': 1, 'a survey showed': 1, 'survey showed yesterday': 1, 'showed yesterday </s>': 1, '<s> two three': 27, 'two three dentists': 1, 'three dentists said': 1, 'dentists said they': 1, 'said they had': 9, 'they had extracted': 2, 'had extracted teeth': 1, 'extracted teeth that': 1, 'teeth that they': 1, 'that they might': 12, 'they might have': 21, 'might have filled': 1, 'have filled before': 1, 'filled before the': 1, 'before the contract': 1, 'the contract was': 1, 'contract was introduced': 1, 'was introduced </s>': 1, '<s> more than': 17, 'more than half': 19, 'than half said': 1, 'half said their': 1, 'said their pay': 1, 'their pay last': 1, 'pay last year': 1, 'last year had': 2, 'year had not': 1, 'had not increased': 1, 'not increased or': 1, 'increased or even': 1, 'or even fell': 1, 'even fell </s>': 1, '<s> the survey': 14, 'the survey of': 3, 'survey of 866': 1, 'of 866 dentists': 1, '866 dentists in': 1, 'dentists in the': 1, 'in the dental': 1, 'the dental journal': 1, 'dental journal the': 1, 'journal the probe': 1, 'the probe says': 1, 'probe says when': 1, 'says when dentists': 1, 'when dentists were': 1, 'dentists were asked': 1, 'were asked whether': 1, 'asked whether they': 1, 'whether they had': 6, 'had extracted or': 1, 'extracted or referred': 1, 'or referred for': 1, 'referred for extraction': 1, 'for extraction any': 1, 'extraction any teeth': 1, 'any teeth which': 1, 'teeth which they': 1, 'which they might': 5, 'they might not': 8, 'might not have': 18, 'not have done': 7, 'have done before': 1, 'done before the': 2, 'before the introduction': 3, 'the introduction of': 60}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "vanilla_tri_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_vanilla_dict_tri_sen = dict(build_n_gram_freq_counts(train_corpus_sen, TRIGRAM_VAL))\n",
    "\n",
    "# Variable to represent end time\n",
    "vanilla_tri_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "vanilla_tri_time = vanilla_tri_end_time - vanilla_tri_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_vanilla_dict_tri_sen.items())[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31a4e0b9-e339-40f7-9cdb-23b48e373b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build frequency counts for vanilla models: 6.75 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to build the frequency counts for the vanilla models\n",
    "vanilla_total_time = vanilla_uni_time + vanilla_bi_time + vanilla_tri_time\n",
    "print(f\"Time taken to build frequency counts for vanilla models: {round(vanilla_total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c80d0",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07225730-e90c-4212-a0db-e01718dfb276",
   "metadata": {},
   "source": [
    "### a) Applying Laplace Smoothing to all n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e10bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_laplace(org_dict):\n",
    "    # Creating an empty lists of dictionaries to calculate TF (term frequency)\n",
    "    laplace_dict = org_dict.copy()\n",
    "\n",
    "    # Calculate the frequency of each word in every document\n",
    "    for word, count in laplace_dict.items():\n",
    "        new_count = count+1\n",
    "        laplace_dict[word] = new_count\n",
    "        #laplace_dict.update({word: new_count})\n",
    "\n",
    "\n",
    "    return laplace_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64956e4-027c-4da7-bf27-485f92a11827",
   "metadata": {},
   "source": [
    "### i) Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31309757-eb25-4895-a6d7-7db66c8fdd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 272095, 'struggling': 74, 'dentists': 18, 'pull': 164, 'more': 5770, 'teeth': 205, '</s>': 272095, 'by': 13563, 'david': 545, 'fletcher': 21, 'health': 661, 'services': 456, 'correspondent': 102, 'are': 14998, 'pulling': 108, 'out': 6092, 'patients': 629, 'unnecessarily': 7, 'as': 15482, 'they': 14491, 'struggle': 93, 'to': 74112, 'maintain': 92, 'living': 546, 'standards': 150, 'under': 1443, 'a': 70176, 'new': 2967, 'government': 1085, 'contract': 199, 'survey': 184, 'showed': 340, 'yesterday': 1254, 'two': 6510, 'three': 3424, 'said': 10249, 'had': 14931, 'extracted': 24, 'that': 35412, 'might': 2392, 'have': 17788, 'filled': 138, 'before': 2825, 'the': 169626, 'was': 28452, 'introduced': 199, 'than': 3167, 'half': 1417, 'their': 6602, 'pay': 676, 'last': 2992, 'year': 1911, 'not': 15056, 'increased': 307, 'or': 10359, 'even': 2195, 'fell': 379, 'of': 76796, '866': 4, 'in': 53569, 'dental': 14, 'journal': 30, 'probe': 40, 'says': 1297, 'when': 7292, 'were': 9754, 'asked': 1109, 'whether': 1003, 'referred': 136, 'for': 24429, 'extraction': 23, 'any': 3616, 'which': 9696, 'done': 1648, 'introduction': 150, '1990': 172, '61': 52, 'while': 1714, 'nearly': 502, '37': 48, 'mr': 1983, 'jeremy': 29, 'cowan': 5, 'editor': 66, 'contracts': 47, 'practical': 155, 'effect': 588, 'reduce': 189, 'preventive': 30, 'dentistry': 2, 'extractions': 2, 'conclusions': 69, 'challenged': 37, 'joe': 285, 'rich': 177, 'chairman': 310, 'british': 896, 'associations': 50, 'general': 896, 'committee': 298}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "laplace_uni_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the word and its frequency count\n",
    "train_laplace_dict_sen = apply_laplace(train_vanilla_dict_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "laplace_uni_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "laplace_uni_time = laplace_uni_end_time - laplace_uni_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(print(dict(list(train_laplace_dict_sen.items())[0:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d159c-0177-4136-ba21-0ff70f9fed8d",
   "metadata": {},
   "source": [
    "### ii) Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fb5b134-832e-4e5c-90bf-99cbf90b0602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling': 5, 'struggling dentists': 2, 'dentists pull': 2, 'pull more': 2, 'more teeth': 5, 'teeth </s>': 59, '<s> by': 1091, 'by david': 55, 'david fletcher': 2, 'fletcher health': 2, 'health services': 32, 'services correspondent': 13, 'correspondent </s>': 90, '<s> dentists': 3, 'dentists are': 2, 'are pulling': 4, 'pulling out': 11, 'out patients': 2, 'patients teeth': 2, 'teeth unnecessarily': 2, 'unnecessarily as': 2, 'as they': 400, 'they struggle': 2, 'struggle to': 13, 'to maintain': 55, 'maintain living': 2, 'living standards': 8, 'standards under': 2, 'under a': 98, 'a new': 696, 'new government': 7, 'government contract': 2, 'contract a': 4, 'a survey': 32, 'survey showed': 5, 'showed yesterday': 2, 'yesterday </s>': 343, '<s> two': 620, 'two three': 536, 'three dentists': 2, 'dentists said': 2, 'said they': 110, 'they had': 753, 'had extracted': 4, 'extracted teeth': 2, 'teeth that': 3, 'that they': 641, 'they might': 116, 'might have': 366, 'have filled': 2, 'filled before': 2, 'before the': 443, 'the contract': 22, 'contract was': 3, 'was introduced': 19, 'introduced </s>': 14, '<s> more': 202, 'more than': 603, 'than half': 38, 'half said': 3, 'said their': 9, 'their pay': 5, 'pay last': 2, 'last year': 378, 'year had': 3, 'had not': 319, 'not increased': 3, 'increased or': 2, 'or even': 144, 'even fell': 2, 'fell </s>': 18, '<s> the': 16591, 'the survey': 37, 'survey of': 25, 'of 866': 2, '866 dentists': 2, 'dentists in': 2, 'in the': 15043, 'the dental': 3, 'dental journal': 2, 'journal the': 2, 'the probe': 2, 'probe says': 2, 'says when': 9, 'when dentists': 2, 'dentists were': 4, 'were asked': 18, 'asked whether': 9, 'whether they': 72, 'extracted or': 2, 'or referred': 3, 'referred for': 3, 'for extraction': 2, 'extraction any': 2, 'any teeth': 2, 'teeth which': 3, 'which they': 210, 'might not': 105, 'not have': 255, 'have done': 203}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "laplace_bi_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_laplace_dict_bi_sen = apply_laplace(train_vanilla_dict_bi_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "laplace_bi_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "laplace_bi_time = laplace_bi_end_time - laplace_bi_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_laplace_dict_bi_sen.items())[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190a5a2-208d-45a0-89ad-b61cee25a89b",
   "metadata": {},
   "source": [
    "### iii) Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47206889-2293-48e8-8066-3cb7cf4c08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling dentists': 2, 'struggling dentists pull': 2, 'dentists pull more': 2, 'pull more teeth': 2, 'more teeth </s>': 2, '<s> by david': 43, 'by david fletcher': 2, 'david fletcher health': 2, 'fletcher health services': 2, 'health services correspondent': 13, 'services correspondent </s>': 13, '<s> dentists are': 2, 'dentists are pulling': 2, 'are pulling out': 4, 'pulling out patients': 2, 'out patients teeth': 2, 'patients teeth unnecessarily': 2, 'teeth unnecessarily as': 2, 'unnecessarily as they': 2, 'as they struggle': 2, 'they struggle to': 2, 'struggle to maintain': 2, 'to maintain living': 2, 'maintain living standards': 2, 'living standards under': 2, 'standards under a': 2, 'under a new': 6, 'a new government': 4, 'new government contract': 2, 'government contract a': 2, 'contract a survey': 2, 'a survey showed': 2, 'survey showed yesterday': 2, 'showed yesterday </s>': 2, '<s> two three': 28, 'two three dentists': 2, 'three dentists said': 2, 'dentists said they': 2, 'said they had': 10, 'they had extracted': 3, 'had extracted teeth': 2, 'extracted teeth that': 2, 'teeth that they': 2, 'that they might': 13, 'they might have': 22, 'might have filled': 2, 'have filled before': 2, 'filled before the': 2, 'before the contract': 2, 'the contract was': 2, 'contract was introduced': 2, 'was introduced </s>': 2, '<s> more than': 18, 'more than half': 20, 'than half said': 2, 'half said their': 2, 'said their pay': 2, 'their pay last': 2, 'pay last year': 2, 'last year had': 3, 'year had not': 2, 'had not increased': 2, 'not increased or': 2, 'increased or even': 2, 'or even fell': 2, 'even fell </s>': 2, '<s> the survey': 15, 'the survey of': 4, 'survey of 866': 2, 'of 866 dentists': 2, '866 dentists in': 2, 'dentists in the': 2, 'in the dental': 2, 'the dental journal': 2, 'dental journal the': 2, 'journal the probe': 2, 'the probe says': 2, 'probe says when': 2, 'says when dentists': 2, 'when dentists were': 2, 'dentists were asked': 2, 'were asked whether': 2, 'asked whether they': 2, 'whether they had': 7, 'had extracted or': 2, 'extracted or referred': 2, 'or referred for': 2, 'referred for extraction': 2, 'for extraction any': 2, 'extraction any teeth': 2, 'any teeth which': 2, 'teeth which they': 2, 'which they might': 6, 'they might not': 9, 'might not have': 19, 'not have done': 8, 'have done before': 2, 'done before the': 3, 'before the introduction': 4, 'the introduction of': 61}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "laplace_tri_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_laplace_dict_tri_sen = apply_laplace(train_vanilla_dict_tri_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "laplace_tri_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "laplace_tri_time = laplace_tri_end_time - laplace_tri_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_laplace_dict_tri_sen.items())[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05211a99-e4a3-4583-98b4-d3292bb068ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build frequency counts for laplace models: 0.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to build the frequency counts for the laplace models\n",
    "laplace_total_time = laplace_uni_time + laplace_bi_time + laplace_tri_time\n",
    "print(f\"Time taken to build frequency counts for laplace models: {round(laplace_total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb690e-a701-4a09-a4c6-f181fbd69e90",
   "metadata": {},
   "source": [
    "### bI) Applying < UNK > tags to any word counts less than or equal to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3dfaa319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_unk(org_dict):\n",
    "    unk_dict = org_dict.copy()\n",
    "    unk_words = []\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word, count in unk_dict.items():\n",
    "        if count <= 2:\n",
    "            unk_count += count\n",
    "            unk_words.append(word)\n",
    "    \n",
    "    for x in unk_words:\n",
    "        unk_dict.pop(x)\n",
    "    \n",
    "    unk_dict[\"UNK\"] = unk_count\n",
    "    \n",
    "    return unk_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14baaad2-f210-45c0-ba82-da900a52c5c0",
   "metadata": {},
   "source": [
    "### i) Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95e274a0-7ab3-4705-99aa-ed889e5692d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 272094, 'struggling': 73, 'dentists': 17, 'pull': 163, 'more': 5769, 'teeth': 204, '</s>': 272094, 'by': 13562, 'david': 544, 'fletcher': 20, 'health': 660, 'services': 455, 'correspondent': 101, 'are': 14997, 'pulling': 107, 'out': 6091, 'patients': 628, 'unnecessarily': 6, 'as': 15481, 'they': 14490, 'struggle': 92, 'to': 74111, 'maintain': 91, 'living': 545, 'standards': 149, 'under': 1442, 'a': 70175, 'new': 2966, 'government': 1084, 'contract': 198, 'survey': 183, 'showed': 339, 'yesterday': 1253, 'two': 6509, 'three': 3423, 'said': 10248, 'had': 14930, 'extracted': 23, 'that': 35411, 'might': 2391, 'have': 17787, 'filled': 137, 'before': 2824, 'the': 169625, 'was': 28451, 'introduced': 198, 'than': 3166, 'half': 1416, 'their': 6601, 'pay': 675, 'last': 2991, 'year': 1910, 'not': 15055, 'increased': 306, 'or': 10358, 'even': 2194, 'fell': 378, 'of': 76795, '866': 3, 'in': 53568, 'dental': 13, 'journal': 29, 'probe': 39, 'says': 1296, 'when': 7291, 'were': 9753, 'asked': 1108, 'whether': 1002, 'referred': 135, 'for': 24428, 'extraction': 22, 'any': 3615, 'which': 9695, 'done': 1647, 'introduction': 149, '1990': 171, '61': 51, 'while': 1713, 'nearly': 501, '37': 47, 'mr': 1982, 'jeremy': 28, 'cowan': 4, 'editor': 65, 'contracts': 46, 'practical': 154, 'effect': 587, 'reduce': 188, 'preventive': 29, 'conclusions': 68, 'challenged': 36, 'joe': 284, 'rich': 176, 'chairman': 309, 'british': 895, 'associations': 49, 'general': 895, 'committee': 297, 'who': 5682, 'it': 39565}\n",
      "{'UNK': 53754}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_uni_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the word and its frequency count\n",
    "train_unk_dict_sen = apply_unk(train_vanilla_dict_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_uni_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_uni_time = unk_uni_end_time - unk_uni_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_dict_sen.items())[0:100]))\n",
    "for key, val in train_unk_dict_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29fd47-fcad-4856-9fbd-0cd1b8c1c27f",
   "metadata": {},
   "source": [
    "### ii) Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b953465b-562f-4755-9f29-3e1de357d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling': 4, 'more teeth': 4, 'teeth </s>': 58, '<s> by': 1090, 'by david': 54, 'health services': 31, 'services correspondent': 12, 'correspondent </s>': 89, 'are pulling': 3, 'pulling out': 10, 'as they': 399, 'struggle to': 12, 'to maintain': 54, 'living standards': 7, 'under a': 97, 'a new': 695, 'new government': 6, 'contract a': 3, 'a survey': 31, 'survey showed': 4, 'yesterday </s>': 342, '<s> two': 619, 'two three': 535, 'said they': 109, 'they had': 752, 'had extracted': 3, 'that they': 640, 'they might': 115, 'might have': 365, 'before the': 442, 'the contract': 21, 'was introduced': 18, 'introduced </s>': 13, '<s> more': 201, 'more than': 602, 'than half': 37, 'said their': 8, 'their pay': 4, 'last year': 377, 'had not': 318, 'or even': 143, 'fell </s>': 17, '<s> the': 16590, 'the survey': 36, 'survey of': 24, 'in the': 15042, 'says when': 8, 'dentists were': 3, 'were asked': 17, 'asked whether': 8, 'whether they': 71, 'which they': 209, 'might not': 104, 'not have': 254, 'have done': 202, 'done before': 9, 'the introduction': 69, 'introduction of': 70, 'of the': 17410, 'the 1990': 9, 'not </s>': 724, '<s> mr': 717, 'said the': 565, 'the contracts': 3, 'practical effect': 3, 'effect was': 12, 'was to': 363, 'to reduce': 100, 'the conclusions': 6, 'were challenged': 3, 'challenged by': 4, 'by mr': 47, 'mr joe': 4, 'chairman of': 95, 'the british': 271, 'services committee': 8, 'who said': 57, 'said it': 287, 'it was': 4810, 'was probably': 56, 'true that': 46, 'that more': 26, 'teeth were': 7, 'were being': 55, 'were seeing': 5, 'more patients': 5, 'patients </s>': 51, 'had no': 257, 'no financial': 3, 'incentive to': 5, 'to extract': 17, 'he said': 1793, 'said </s>': 1522, 'fee was': 5, 'but the': 1172, 'the filling': 4, 'could be': 830, 'be almost': 10, 'almost anything': 9, 'anything the': 13}\n",
      "{'UNK': 895347}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_bi_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_unk_dict_bi_sen = apply_unk(train_vanilla_dict_bi_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_bi_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_bi_time = unk_bi_end_time - unk_bi_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_dict_bi_sen.items())[0:100]))\n",
    "for key, val in train_unk_dict_bi_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0dc963-79b5-4115-8575-cca04716a4e9",
   "metadata": {},
   "source": [
    "### iii) Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2888f75-47b8-4dfd-83db-3169878fef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> by david': 42, 'health services correspondent': 12, 'services correspondent </s>': 12, 'are pulling out': 3, 'under a new': 5, 'a new government': 3, '<s> two three': 27, 'said they had': 9, 'that they might': 12, 'they might have': 21, '<s> more than': 17, 'more than half': 19, '<s> the survey': 14, 'the survey of': 3, 'whether they had': 6, 'which they might': 5, 'they might not': 8, 'might not have': 18, 'not have done': 7, 'before the introduction': 3, 'the introduction of': 60, 'introduction of the': 18, 'they had not': 14, 'had not </s>': 5, 'effect was to': 3, 'chairman of the': 57, 'of the british': 63, 'who said it': 6, 'said it was': 105, 'it was probably': 12, 'more patients </s>': 3, 'he said </s>': 469, 'was being made': 4, 'by the government': 23, 'the government to': 48, 'to implement the': 6, 'and he had': 63, 'to arrange a': 6, 'a meeting </s>': 7, 'to appeal </s>': 6, 'to appeal against': 3, 'appeal against his': 4, 'was found guilty': 4, 'by the general': 6, 'the general medical': 7, 'general medical council': 6, 'heard that he': 3, 'he failed to': 5, 'to detect a': 4, '<s> the nhs': 4, 'the nhs is': 4, 'is not for': 7, 'not for sale': 5, 'in her first': 3, 'the cabinet </s>': 5, '<s> by george': 7, '<s> the new': 70, 'mrs virginia bottomley': 3, 'to take the': 105, 'the national health': 14, 'national health service': 15, 'health service </s>': 6, '<s> but her': 13, 'to provide a': 45, 'a period of': 43, 'employed by the': 4, 'by the nhs': 3, 'coupled with a': 5, 'with a fierce': 3, 'to maintain the': 12, 'the overwhelming majority': 6, 'overwhelming majority of': 8, 'to turn the': 10, 'the general election': 21, 'a mandate to': 3, 'to reverse the': 6, '<s> mrs bottomley': 6, 'the opportunity to': 43, 'and to give': 8, 'the confidence to': 10, 'to make them': 17, '<s> in her': 33, 'she said the': 16, 'the election result': 5, 'commitment to the': 18, 'to the nhs': 3, 'of health care': 11, '<s> it is': 1425, 'it is not': 302, 'for sale </s>': 27, '<s> it will': 135, 'it will continue': 5, 'will continue to': 31, 'continue to be': 16, 'to be true': 8, 'be true to': 3, 'available to all': 3, 'at the point': 17, 'the point of': 77, 'she said </s>': 228}\n",
      "{'UNK': 2102272}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_tri_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_unk_dict_tri_sen = apply_unk(train_vanilla_dict_tri_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_tri_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_tri_time = unk_tri_end_time - unk_tri_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_dict_tri_sen.items())[0:100]))\n",
    "for key, val in train_unk_dict_tri_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe9aeaab-6af0-4335-93fc-0d02f77b9707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build frequency counts for UNK models: 0.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to build the frequency counts for the UNK models\n",
    "unk_total_time = unk_uni_time + unk_bi_time + unk_tri_time\n",
    "print(f\"Time taken to build frequency counts for UNK models: {round(unk_total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58077431-26bb-426c-859b-a55cc6424f53",
   "metadata": {},
   "source": [
    "### bII) Applying Laplace smoothing to UNK dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6ec3c-0599-44c2-8c9a-753222993bd7",
   "metadata": {},
   "source": [
    "### i) Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "add1f714-af26-4af9-83c4-7a5d99ae6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 272095, 'struggling': 74, 'dentists': 18, 'pull': 164, 'more': 5770, 'teeth': 205, '</s>': 272095, 'by': 13563, 'david': 545, 'fletcher': 21, 'health': 661, 'services': 456, 'correspondent': 102, 'are': 14998, 'pulling': 108, 'out': 6092, 'patients': 629, 'unnecessarily': 7, 'as': 15482, 'they': 14491, 'struggle': 93, 'to': 74112, 'maintain': 92, 'living': 546, 'standards': 150, 'under': 1443, 'a': 70176, 'new': 2967, 'government': 1085, 'contract': 199, 'survey': 184, 'showed': 340, 'yesterday': 1254, 'two': 6510, 'three': 3424, 'said': 10249, 'had': 14931, 'extracted': 24, 'that': 35412, 'might': 2392, 'have': 17788, 'filled': 138, 'before': 2825, 'the': 169626, 'was': 28452, 'introduced': 199, 'than': 3167, 'half': 1417, 'their': 6602, 'pay': 676, 'last': 2992, 'year': 1911, 'not': 15056, 'increased': 307, 'or': 10359, 'even': 2195, 'fell': 379, 'of': 76796, '866': 4, 'in': 53569, 'dental': 14, 'journal': 30, 'probe': 40, 'says': 1297, 'when': 7292, 'were': 9754, 'asked': 1109, 'whether': 1003, 'referred': 136, 'for': 24429, 'extraction': 23, 'any': 3616, 'which': 9696, 'done': 1648, 'introduction': 150, '1990': 172, '61': 52, 'while': 1714, 'nearly': 502, '37': 48, 'mr': 1983, 'jeremy': 29, 'cowan': 5, 'editor': 66, 'contracts': 47, 'practical': 155, 'effect': 588, 'reduce': 189, 'preventive': 30, 'conclusions': 69, 'challenged': 37, 'joe': 285, 'rich': 177, 'chairman': 310, 'british': 896, 'associations': 50, 'general': 896, 'committee': 298, 'who': 5683, 'it': 39566}\n",
      "{'UNK': 53755}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_laplace_uni_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the word and its frequency count\n",
    "train_unk_laplace_dict_sen = apply_laplace(train_unk_dict_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_laplace_uni_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_laplace_uni_time = unk_laplace_uni_end_time - unk_laplace_uni_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_laplace_dict_sen.items())[0:100]))\n",
    "for key, val in train_unk_laplace_dict_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187f4a0-79b4-4a6a-b9ca-65c4511ff0ac",
   "metadata": {},
   "source": [
    "### ii) Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f681739-02e7-403b-9050-e685903ce20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> struggling': 5, 'more teeth': 5, 'teeth </s>': 59, '<s> by': 1091, 'by david': 55, 'health services': 32, 'services correspondent': 13, 'correspondent </s>': 90, 'are pulling': 4, 'pulling out': 11, 'as they': 400, 'struggle to': 13, 'to maintain': 55, 'living standards': 8, 'under a': 98, 'a new': 696, 'new government': 7, 'contract a': 4, 'a survey': 32, 'survey showed': 5, 'yesterday </s>': 343, '<s> two': 620, 'two three': 536, 'said they': 110, 'they had': 753, 'had extracted': 4, 'that they': 641, 'they might': 116, 'might have': 366, 'before the': 443, 'the contract': 22, 'was introduced': 19, 'introduced </s>': 14, '<s> more': 202, 'more than': 603, 'than half': 38, 'said their': 9, 'their pay': 5, 'last year': 378, 'had not': 319, 'or even': 144, 'fell </s>': 18, '<s> the': 16591, 'the survey': 37, 'survey of': 25, 'in the': 15043, 'says when': 9, 'dentists were': 4, 'were asked': 18, 'asked whether': 9, 'whether they': 72, 'which they': 210, 'might not': 105, 'not have': 255, 'have done': 203, 'done before': 10, 'the introduction': 70, 'introduction of': 71, 'of the': 17411, 'the 1990': 10, 'not </s>': 725, '<s> mr': 718, 'said the': 566, 'the contracts': 4, 'practical effect': 4, 'effect was': 13, 'was to': 364, 'to reduce': 101, 'the conclusions': 7, 'were challenged': 4, 'challenged by': 5, 'by mr': 48, 'mr joe': 5, 'chairman of': 96, 'the british': 272, 'services committee': 9, 'who said': 58, 'said it': 288, 'it was': 4811, 'was probably': 57, 'true that': 47, 'that more': 27, 'teeth were': 8, 'were being': 56, 'were seeing': 6, 'more patients': 6, 'patients </s>': 52, 'had no': 258, 'no financial': 4, 'incentive to': 6, 'to extract': 18, 'he said': 1794, 'said </s>': 1523, 'fee was': 6, 'but the': 1173, 'the filling': 5, 'could be': 831, 'be almost': 11, 'almost anything': 10, 'anything the': 14}\n",
      "{'UNK': 895348}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_laplace_bi_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_unk_laplace_dict_bi_sen = apply_laplace(train_unk_dict_bi_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_laplace_bi_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_laplace_bi_time = unk_laplace_bi_end_time - unk_laplace_bi_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_laplace_dict_bi_sen.items())[0:100]))\n",
    "for key, val in train_unk_laplace_dict_bi_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22664f-5de5-44cc-8562-336d6b4934dc",
   "metadata": {},
   "source": [
    "### iii) Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59c2fe67-7364-44f3-8b52-3f23164ee2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s> by david': 43, 'health services correspondent': 13, 'services correspondent </s>': 13, 'are pulling out': 4, 'under a new': 6, 'a new government': 4, '<s> two three': 28, 'said they had': 10, 'that they might': 13, 'they might have': 22, '<s> more than': 18, 'more than half': 20, '<s> the survey': 15, 'the survey of': 4, 'whether they had': 7, 'which they might': 6, 'they might not': 9, 'might not have': 19, 'not have done': 8, 'before the introduction': 4, 'the introduction of': 61, 'introduction of the': 19, 'they had not': 15, 'had not </s>': 6, 'effect was to': 4, 'chairman of the': 58, 'of the british': 64, 'who said it': 7, 'said it was': 106, 'it was probably': 13, 'more patients </s>': 4, 'he said </s>': 470, 'was being made': 5, 'by the government': 24, 'the government to': 49, 'to implement the': 7, 'and he had': 64, 'to arrange a': 7, 'a meeting </s>': 8, 'to appeal </s>': 7, 'to appeal against': 4, 'appeal against his': 5, 'was found guilty': 5, 'by the general': 7, 'the general medical': 8, 'general medical council': 7, 'heard that he': 4, 'he failed to': 6, 'to detect a': 5, '<s> the nhs': 5, 'the nhs is': 5, 'is not for': 8, 'not for sale': 6, 'in her first': 4, 'the cabinet </s>': 6, '<s> by george': 8, '<s> the new': 71, 'mrs virginia bottomley': 4, 'to take the': 106, 'the national health': 15, 'national health service': 16, 'health service </s>': 7, '<s> but her': 14, 'to provide a': 46, 'a period of': 44, 'employed by the': 5, 'by the nhs': 4, 'coupled with a': 6, 'with a fierce': 4, 'to maintain the': 13, 'the overwhelming majority': 7, 'overwhelming majority of': 9, 'to turn the': 11, 'the general election': 22, 'a mandate to': 4, 'to reverse the': 7, '<s> mrs bottomley': 7, 'the opportunity to': 44, 'and to give': 9, 'the confidence to': 11, 'to make them': 18, '<s> in her': 34, 'she said the': 17, 'the election result': 6, 'commitment to the': 19, 'to the nhs': 4, 'of health care': 12, '<s> it is': 1426, 'it is not': 303, 'for sale </s>': 28, '<s> it will': 136, 'it will continue': 6, 'will continue to': 32, 'continue to be': 17, 'to be true': 9, 'be true to': 4, 'available to all': 4, 'at the point': 18, 'the point of': 78, 'she said </s>': 229}\n",
      "{'UNK': 2102273}\n"
     ]
    }
   ],
   "source": [
    "# Variable to represent start time\n",
    "unk_laplace_tri_start_time = time.time()\n",
    "\n",
    "# Creating an empty dictionary to store the phrase and its frequency count\n",
    "train_unk_laplace_dict_tri_sen = apply_laplace(train_unk_dict_tri_sen)\n",
    "\n",
    "# Variable to represent end time\n",
    "unk_laplace_tri_end_time = time.time()\n",
    "\n",
    "# Calculate actual time\n",
    "unk_laplace_tri_time = unk_laplace_tri_end_time - unk_laplace_tri_start_time\n",
    "\n",
    "# Printing the contents of the dictionary (for testing purposes)\n",
    "print(dict(list(train_unk_laplace_dict_tri_sen.items())[0:100]))\n",
    "for key, val in train_unk_laplace_dict_tri_sen.items():\n",
    "    if key == 'UNK':\n",
    "        print({key: val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ccb1941-8874-4eb0-ac81-e613d9f1f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build frequency counts for UNK+laplace models: 0.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to build the frequency counts for the UNK+laplace models\n",
    "unk_laplace_total_time = unk_laplace_uni_time + unk_laplace_bi_time + unk_laplace_tri_time\n",
    "print(f\"Time taken to build frequency counts for UNK+laplace models: {round(unk_laplace_total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "393d2514-a5c7-43ee-9a85-2419671181fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to build frequency counts for all models: 8.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Checking total time it took to build the frequency counts for all models\n",
    "models_total_time = vanilla_total_time + laplace_total_time + unk_total_time + unk_laplace_total_time\n",
    "print(f\"Time taken to build frequency counts for all models: {round(models_total_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "618f41fa-624b-4d3a-91ae-5bce205eb474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after building language models: 752.0703125 MB\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "print(f\"Memory usage after building language models: {process.memory_info().rss / (1024 * 1024)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4554d-ad7a-4611-b0ce-a815dbf04e06",
   "metadata": {},
   "source": [
    "### Saving generated frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "352b2c07-4b78-404f-a249-1ddc352b5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename): \n",
    "    with open(filename, 'wb') as file: \n",
    "        pickle.dump(model, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eee9c750-58e9-496d-bb84-c4fcaa4385ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(train_vanilla_dict_sen, vanilla_dict_filename)\n",
    "save_model(train_vanilla_dict_bi_sen, vanilla_dict_bi_filename)\n",
    "save_model(train_vanilla_dict_tri_sen, vanilla_dict_tri_filename)\n",
    "save_model(train_laplace_dict_sen, laplace_dict_filename)\n",
    "save_model(train_laplace_dict_bi_sen, laplace_dict_bi_filename)\n",
    "save_model(train_laplace_dict_tri_sen, laplace_dict_tri_filename)\n",
    "save_model(train_unk_dict_sen, unk_dict_filename)\n",
    "save_model(train_unk_dict_bi_sen, unk_dict_bi_filename)\n",
    "save_model(train_unk_dict_tri_sen, unk_dict_tri_filename)\n",
    "save_model(train_unk_laplace_dict_sen, unk_laplace_dict_filename)\n",
    "save_model(train_unk_laplace_dict_bi_sen, unk_laplace_dict_bi_filename)\n",
    "save_model(train_unk_laplace_dict_tri_sen, unk_laplace_dict_tri_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44ef4ff8-33b4-4ccb-b1e2-9b6fca3cef0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vanilla_bi_model.pkl', 'vanilla_tri_model.pkl', 'vanilla_uni_model.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('models/vanilla')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a247f-5589-4b0d-8cae-e56412a26e0e",
   "metadata": {},
   "source": [
    "### Loading training and test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da90caa2-5546-4a42-8cda-60c57cbcf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        model_to_get = pickle.load(file) \n",
    "\n",
    "    return model_to_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90d49137-aa49-469b-bb4c-e4a69fd47e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vanilla_dict_sen = load_model(vanilla_dict_filename)\n",
    "train_vanilla_dict_bi_sen = load_model(vanilla_dict_bi_filename)\n",
    "train_vanilla_dict_tri_sen = load_model(vanilla_dict_tri_filename)\n",
    "train_laplace_dict_sen = load_model(laplace_dict_filename)\n",
    "train_laplace_dict_bi_sen = load_model(laplace_dict_bi_filename)\n",
    "train_laplace_dict_tri_sen = load_model(laplace_dict_tri_filename)\n",
    "train_unk_dict_sen = load_model(unk_dict_filename)\n",
    "train_unk_dict_bi_sen = load_model(unk_dict_bi_filename)\n",
    "train_unk_dict_tri_sen = load_model(unk_dict_tri_filename)\n",
    "train_unk_laplace_dict_sen = load_model(unk_laplace_dict_filename)\n",
    "train_unk_laplace_dict_bi_sen = load_model(unk_laplace_dict_bi_filename)\n",
    "train_unk_laplace_dict_tri_sen = load_model(unk_laplace_dict_tri_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72ee23c7-c530-4d6f-be86-fa642ce055eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(list(train_vanilla_dict_sen.items())[0:150]) (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3824a3b-eb35-42aa-83b7-93767bba1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(list(train_laplace_dict_bi_sen.items())[0:150]) (for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc6c1cda-e882-4214-a688-332d67781ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_unk_dict_tri_sen (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffc021-1eef-4263-b9d5-87a6b27cc8c3",
   "metadata": {},
   "source": [
    "### c) Removing punctuation from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b997b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list to store the sentences of the train corpus for easy access\n",
    "train_extracted_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8448d974-142f-436e-8bd3-b0233cbea223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the corpus and moving the sentences to the new list, while removing any punctuation marks that may appear\n",
    "for i in range(len(train_corpus_sen)):\n",
    "    for j in range(len(train_corpus_sen[i])):\n",
    "        text_words = train_corpus_sen[i][j].split()\n",
    "        text_words_string = ' '.join(text_words[1:-1])\n",
    "        cleaned_text = \"<s> \" + remove_punctuation(text_words_string) + \" </s>\"\n",
    "        train_extracted_sentences.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5938ca56-9e62-4872-97b6-a49b5958627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> struggling dentists pull more teeth </s>',\n",
       " '<s> by david fletcher health services correspondent </s>',\n",
       " '<s> dentists are pulling out patients teeth unnecessarily as they struggle to maintain living standards under a new government contract a survey showed yesterday </s>',\n",
       " '<s> two three dentists said they had extracted teeth that they might have filled before the contract was introduced </s>',\n",
       " '<s> more than half said their pay last year had not increased or even fell </s>',\n",
       " '<s> the survey of 866 dentists in the dental journal the probe says when dentists were asked whether they had extracted or referred for extraction any teeth which they might not have done before the introduction of the 1990 contract 61 said they had while nearly 37 said they had not </s>',\n",
       " '<s> mr jeremy cowan journal editor said the contracts practical effect was to reduce preventive dentistry more extractions </s>',\n",
       " '<s> the conclusions were challenged by mr joe rich chairman of the british dental associations general dental services committee who said it was probably true that more teeth were being extracted because dentists were seeing more patients </s>',\n",
       " '<s> dentists had no financial incentive to extract teeth he said </s>',\n",
       " '<s> the extraction fee was 9 but the filling fee could be almost anything the work needed </s>',\n",
       " '<s> mr rich said many dentists were concerned that not enough money was being made available by the government to implement the contract and he had approached mrs bottomley health secretary to arrange a meeting </s>',\n",
       " '<s> misconduct case gp to appeal </s>',\n",
       " '<s> dr robert jones the essex general practitioner suspended for eight months for serious professional misconduct is to appeal against his conviction </s>',\n",
       " '<s> dr jones of coggeshall was found guilty by the general medical council last month which heard that he failed to detect a serious case of appendicitis prescribing instead indigestion and ulcer tablets </s>',\n",
       " '<s> the nhs is not for sale virginia bottomley talks to george jones in her first interview since joining the cabinet </s>',\n",
       " '<s> by george jones </s>',\n",
       " '<s> the new health secretary mrs virginia bottomley set herself an ambitious target yesterday to take the politics the national health service </s>',\n",
       " '<s> but her desire to provide a period of stability for nearly one million people employed by the nhs is coupled with a fierce determination to maintain the momentum of reform with the overwhelming majority of hospitals becoming selfgoverning trusts by the mid1990s </s>',\n",
       " '<s> labour sought to turn the general election into a referendum on the nhs asking voters for a mandate to reverse the changes </s>',\n",
       " '<s> mrs bottomley is convinced the tory victory provides the opportunity to entrench the reforms and to give doctors nurses and managers the confidence to make them work </s>',\n",
       " '<s> in her first full interview since being appointed health secretary she said the election result and mr majors unequivocal commitment to the nhs had finally nailed the lie about privatisation of health care </s>',\n",
       " '<s> the nhs is not for profit </s>',\n",
       " '<s> it is not for sale </s>',\n",
       " '<s> it will continue to be true to its founding principles available to all and free at the point of delivery she said </s>',\n",
       " '<s> she recalled a promise made by mr major when he became prime minister that he would work for a nation at ease with itself </s>']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing a subset for testing purposes\n",
    "train_extracted_sentences[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23897100-8907-45fe-8de0-f7ed93e1e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list to store the sentences of the test corpus for easy access\n",
    "test_extracted_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69ed7994-0f04-42fa-ab2c-53c6aeea8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the corpus and moving the sentences to the new list, while removing any punctuation marks that may appear\n",
    "for i in range(len(test_corpus_sen)):\n",
    "    for j in range(len(test_corpus_sen[i])):\n",
    "        text_words = test_corpus_sen[i][j].split()\n",
    "        text_words_string = ' '.join(text_words[1:-1])\n",
    "        cleaned_text = \"<s> \" + remove_punctuation(text_words_string) + \" </s>\"\n",
    "        test_extracted_sentences.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10b4f313-fb8b-4219-95ab-7e4237296190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> property cottage charms </s>',\n",
       " '<s> by sue fieldman </s>',\n",
       " '<s> the reputed birthplace of bob fitzsimmons world heavyweight boxing champion in 1897 is one of the 100 or more properties to be auctioned by the bristol and west between 1820 october </s>',\n",
       " '<s> the house right at 61 wendron street in helston cornwall has a guide price of 5500070000 and a blue plaque in honour of fitzsimmons </s>',\n",
       " '<s> it will be auctioned at the alverton manor hotel truro on 19 october at 3pm </s>',\n",
       " '<s> ferrymans cottage left at noss mayo in south devon is to be auctioned on its own by strutt parker 0392 215631 on 26 october </s>',\n",
       " '<s> the twobedroom cottage needs refurbishment but it is in a superb location in the middle of woodland owned by the national trust and overlooking the river yealm </s>',\n",
       " '<s> the guide price is 150000 </s>',\n",
       " '<s> property forward planning making proposals influencing the decisionmakers allison flight fills in the background to a planning application </s>',\n",
       " '<s> by allison flight </s>',\n",
       " '<s> the planning system is designed around the idea that people should have a say in what happens to their immediate surroundings </s>',\n",
       " '<s> but how do you find out about whether a planning application has been made in the first place </s>',\n",
       " '<s> whether you are proposing or opposing an application what is the best way to influence the decision that is eventually made </s>',\n",
       " '<s> finding out about an application under section 28 of the town and country planning act 1971 the planning authority usually the local council is supposed to advertise certain planning applications in the local newspaper </s>',\n",
       " '<s> the proposals covered are development in a conservation area works to a listed building or which affect the setting of a conservation area or a listed building and developments in areas of outstanding natural beauty </s>',\n",
       " '<s> the planning authority also has to advertise locally any proposals involving large buildings those 20 metres high or any unneighbourly uses a casino scrap yard cemetery or slaughterhouse and put up a notice on the site itself </s>',\n",
       " '<s> some local authorities publish a list of all their new planning applications in the local paper </s>',\n",
       " '<s> often they circulate these lists to local conservation and amenity groups residents associations and subscribers </s>',\n",
       " '<s> ring the local planning authority to find out </s>',\n",
       " '<s> when the council receives a planning application it often consults people who might be affected neighbours other residents and community groups </s>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing a subset for testing purposes\n",
    "test_extracted_sentences[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef01d80-4e01-4196-9178-f32a8a1fa5b4",
   "metadata": {},
   "source": [
    "### d) Calculating Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8966fb96-da5b-4c1a-a728-9dd70315e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd_1 = 0.6\n",
    "lmbd_2 = 0.3\n",
    "lmbd_3 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f8185edf-9e57-4c70-ba04-3585aa83aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(sentence, model, lambdas, tri_dict, bi_dict, uni_dict):\n",
    "    trigram_lambda, bigram_lambda, unigram_lambda = lambdas\n",
    "    words = sentence.split()\n",
    "    #trigram_prob = sum(trigram_probability(words[i-2] + \" \" + words[i-1] + \" \" + words[i], model, tri_dict, bi_dict) for i in range(len(words) - 2))\n",
    "    trigram_prob = reduce(operator.mul, (trigram_probability(words[i-2] + \" \" + words[i-1] + \" \" + words[i], model, tri_dict, bi_dict) for i in range(len(words) - 2)), 1)\n",
    "    #bigram_prob = sum(bigram_probability(words[i-1] + \" \" + words[i], model, bi_dict, uni_dict) for i in range(len(words) - 1))\n",
    "    bigram_prob = reduce(operator.mul, (bigram_probability(words[i-1] + \" \" + words[i], model, bi_dict, uni_dict) for i in range(len(words) - 1)), 1)\n",
    "    #unigram_prob = sum(unigram_probability(word, model, uni_dict) for word in words)\n",
    "    unigram_prob = reduce(operator.mul, (unigram_probability(word, model, uni_dict) for word in words), 1)\n",
    "\n",
    "    probability =  trigram_lambda * trigram_prob + bigram_lambda * bigram_prob + unigram_lambda * unigram_prob\n",
    "\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40660b5c-da94-4210-baf0-0ce1aaab7a6b",
   "metadata": {},
   "source": [
    "### e) Getting n_gram probability (used for linear interpolation, perplexity and sentence probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "728faf8d-faf3-46f7-98fc-7c65ba83f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probability(word, model, unigram_dict):\n",
    "    total_words = len(unigram_dict.keys())\n",
    "    if model == 3:\n",
    "        if word not in unigram_dict.keys():\n",
    "            word_count = unigram_dict.get(\"UNK\", 0.001)\n",
    "        else:\n",
    "            word_count = unigram_dict.get(word.lower(), 0.001)\n",
    "    else:\n",
    "        word_count = unigram_dict.get(word.lower(), 0.001)\n",
    "    return word_count / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f36ced2-5c01-4597-bfd0-c31b7002dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probability(phrase, model, bigram_dict, unigram_dict):\n",
    "    words = phrase.split()\n",
    "    first_word = words[0]\n",
    "    \n",
    "    if model == 3:\n",
    "        if phrase not in bigram_dict.keys():\n",
    "            phrase_count = bigram_dict.get(\"UNK\", 0.001)\n",
    "        else:\n",
    "            phrase_count = bigram_dict.get(phrase.lower(), 0.001)\n",
    "\n",
    "        if first_word not in unigram_dict.keys():\n",
    "            word1_count = unigram_dict.get(\"UNK\", 0.001)\n",
    "        else:\n",
    "            word1_count = unigram_dict.get(first_word.lower(), 0.001)\n",
    "    else:\n",
    "        phrase_count = bigram_dict.get(phrase.lower(), 0.001)\n",
    "        word1_count = unigram_dict.get(first_word.lower(), 0.001)\n",
    "        \n",
    "    return phrase_count / word1_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "581e2480-3f2a-4007-8f11-2e6affb75d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_probability(phrase, model, trigram_dict, bigram_dict):\n",
    "    words = phrase.split()\n",
    "    phrase_2 = words[0] + \" \" + words[1]\n",
    "    \n",
    "    if model == 3:\n",
    "        if phrase not in trigram_dict.keys():\n",
    "            phrase_count = trigram_dict.get(\"UNK\", 0.001)\n",
    "        else:\n",
    "            phrase_count = trigram_dict.get(phrase.lower(), 0.001)\n",
    "\n",
    "        if phrase_2 in bigram_dict.keys():\n",
    "            phrase_2_count = bigram_dict.get(\"UNK\", 0.001)\n",
    "        else:\n",
    "            phrase_2_count = bigram_dict.get(phrase_2.lower(), 0.001)\n",
    "    else:\n",
    "        phrase_count = trigram_dict.get(phrase.lower(), 0.001)\n",
    "        phrase_2_count = bigram_dict.get(phrase_2.lower(), 0.001)\n",
    "        \n",
    "    return phrase_count / phrase_2_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9edba2c4-c5d3-484e-a643-d9af354a7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1428571428756775e-06 1.1428571428598619e-05 6.461032429704854e+37\n"
     ]
    }
   ],
   "source": [
    "li_1 = linear_interpolation(train_extracted_sentences[150], 1, [lmbd_1, lmbd_2, lmbd_3], train_vanilla_dict_tri_sen, train_vanilla_dict_bi_sen, train_vanilla_dict_sen)\n",
    "li_2 = linear_interpolation(train_extracted_sentences[150], 2, [lmbd_1, lmbd_2, lmbd_3], train_laplace_dict_tri_sen, train_laplace_dict_bi_sen, train_laplace_dict_sen)\n",
    "li_3 = linear_interpolation(train_extracted_sentences[150], 3, [lmbd_1, lmbd_2, lmbd_3], train_unk_dict_tri_sen, train_unk_dict_bi_sen, train_unk_dict_sen)\n",
    "print(li_1, li_2, li_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27466dcb-f8b1-4852-825b-98f2db5d61c5",
   "metadata": {},
   "source": [
    "From the above results, it is noted that the linear interpolation from the UNK language models is very high, while the result from the other two types of models is at the proper range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74680e8d-da75-4c42-812f-d471b3265452",
   "metadata": {},
   "source": [
    "### f) Calculating Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51ca49fe-8b69-4936-b604-6b6e5ee06802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, cols = (3, 4)\n",
    "table_vals = [[0 for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "table_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50400ccc-af97-4e65-ac20-8bf86248a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sen, ngram_type, model_num, unigram_model, bigram_model, trigram_model):\n",
    "    words = sen.split()\n",
    "    N = len(words)\n",
    "    total_prob = 1\n",
    "\n",
    "    if ngram_type == 1:\n",
    "        for word in words:\n",
    "            unigram_prob = unigram_probability(word, model_num, unigram_model)\n",
    "            total_prob *= unigram_prob\n",
    "            #print(\"Unigram prob:\", unigram_prob)\n",
    "            #print(\"Total prob:\", total_prob)\n",
    "    elif ngram_type == 2:\n",
    "        for i in range(1, N):\n",
    "            bigram = words[i-1]+\" \"+words[i]\n",
    "            bigram_prob = bigram_probability(bigram, model_num, bigram_model, unigram_model)\n",
    "            total_prob *= bigram_prob\n",
    "            #print(\"Bigram prob:\", bigram_prob)\n",
    "            #print(\"Total prob:\", total_prob)\n",
    "    elif ngram_type == 3:\n",
    "        for i in range(2, N):\n",
    "            trigram = words[i-2]+\" \"+words[i-1]+\" \"+words[i]\n",
    "            trigram_prob = trigram_probability(trigram, model_num, bigram_model, unigram_model)\n",
    "            total_prob *= trigram_prob\n",
    "            #print(\"Trigram prob:\", trigram_prob)\n",
    "            #print(\"Total prob:\", total_prob)\n",
    "\n",
    "    if total_prob > 0:\n",
    "        avg_nll = -(math.log(total_prob))/N\n",
    "    else:\n",
    "        # Handling the case when total_prob is zero\n",
    "        avg_nll = 100\n",
    "    perplexity = np.exp(avg_nll)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "465e4e63-3202-408a-b074-93b3c1e33a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.25067383867051"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_test = train_extracted_sentences[140]\n",
    "\n",
    "sen_perp = calculate_perplexity(sentence_test, 1, 1, train_vanilla_dict_sen, train_vanilla_dict_bi_sen, train_vanilla_dict_tri_sen)\n",
    "sen_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46cde876-07d0-481d-9465-cb097dde5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(test_corpus, unigram_dict, bigram_dict, trigram_dict, lambdas, table_list, index):\n",
    "    perplexity_table = {'Unigram': 0, 'Bigram': 0, 'Trigram': 0, 'Linear Interpolation': 0}\n",
    "    for sentence in test_corpus:\n",
    "        words = sentence.split()\n",
    "        total_words = len(words)\n",
    "        model_num = index+1\n",
    "\n",
    "        trigram_perp = calculate_perplexity(sentence, 3, model_num, unigram_dict, bigram_dict, trigram_dict)\n",
    "        bigram_perp = calculate_perplexity(sentence, 2, model_num, unigram_dict, bigram_dict, trigram_dict)\n",
    "        unigram_perp = calculate_perplexity(sentence, 1, model_num, unigram_dict, bigram_dict, trigram_dict)\n",
    "        interpolation_prob = linear_interpolation(sentence, model_num, lambdas, trigram_dict, bigram_dict, unigram_dict)\n",
    "        \n",
    "        perplexity_table['Unigram'] += unigram_perp\n",
    "        perplexity_table['Bigram'] += bigram_perp\n",
    "        perplexity_table['Trigram'] += trigram_perp\n",
    "        if interpolation_prob > 0:\n",
    "            perplexity_table['Linear Interpolation'] += math.exp(-1 * math.log(interpolation_prob) / total_words)\n",
    "        else:\n",
    "            # Handle the case when interpolation_prob is zero or negative\n",
    "            # For example, set perplexity_table['Linear Interpolation'] to a large value\n",
    "            perplexity_table['Linear Interpolation'] = np.exp(-1 * math.log(0.001) / total_words)\n",
    "    \n",
    "    table_list[index][0] = perplexity_table['Unigram']\n",
    "    table_list[index][1] = perplexity_table['Bigram']\n",
    "    table_list[index][2] = perplexity_table['Trigram']\n",
    "    table_list[index][3] = perplexity_table['Linear Interpolation']\n",
    "    \n",
    "    #perplexity_table = {model: calculate_perplexity(perplexity_table[model]) for model in perplexity_table}\n",
    "    \n",
    "    return table_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0672c94b-90ea-4ddc-811f-d62efe3f8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_models = [train_vanilla_dict_sen, train_vanilla_dict_bi_sen, train_vanilla_dict_tri_sen]\n",
    "laplace_models = [train_laplace_dict_sen, train_laplace_dict_bi_sen, train_laplace_dict_tri_sen]\n",
    "unk_models = [train_unk_dict_sen, train_unk_dict_bi_sen, train_unk_dict_tri_sen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "458b5668-28e3-4b11-9dca-443a504d633a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.6881171418161356e+43, 6.4514811403587235e+44, 60869.0, 681002.7364283614],\n",
       " [2.6881171418161356e+43, 6.4514811403587235e+44, 60869.0, 679251.8956738408],\n",
       " [345928.40067730617,\n",
       "  708000.7574037059,\n",
       "  264.0583362368077,\n",
       "  24505.139273954635]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_num in range(0, 3):\n",
    "    if model_num == 0:\n",
    "        table_vals = fill_table(test_extracted_sentences, vanilla_models[0], vanilla_models[1], vanilla_models[2], [lmbd_1, lmbd_2, lmbd_3], table_vals, 0)\n",
    "    elif model_num == 1:\n",
    "        table_vals = fill_table(test_extracted_sentences, laplace_models[0], laplace_models[1], laplace_models[2], [lmbd_1, lmbd_2, lmbd_3], table_vals, 1)\n",
    "    elif model_num == 2:\n",
    "        table_vals = fill_table(test_extracted_sentences, unk_models[0], unk_models[1], unk_models[2], [lmbd_1, lmbd_2, lmbd_3], table_vals, 2)\n",
    "\n",
    "table_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e5a0c-7887-4316-b457-e40837bff89d",
   "metadata": {},
   "source": [
    "### Saving Results in a DataFrame in order to resemble a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe93315f-1221-4541-b443-ebe902b6c64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unigram</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Linear Interpolation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vanilla</th>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>6.451481e+44</td>\n",
       "      <td>60869.000000</td>\n",
       "      <td>681002.736428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laplace</th>\n",
       "      <td>2.688117e+43</td>\n",
       "      <td>6.451481e+44</td>\n",
       "      <td>60869.000000</td>\n",
       "      <td>679251.895674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNK</th>\n",
       "      <td>3.459284e+05</td>\n",
       "      <td>7.080008e+05</td>\n",
       "      <td>264.058336</td>\n",
       "      <td>24505.139274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Unigram        Bigram       Trigram  Linear Interpolation\n",
       "Vanilla  2.688117e+43  6.451481e+44  60869.000000         681002.736428\n",
       "Laplace  2.688117e+43  6.451481e+44  60869.000000         679251.895674\n",
       "UNK      3.459284e+05  7.080008e+05    264.058336          24505.139274"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp_table_df = pd.DataFrame(table_vals, columns=['Unigram', 'Bigram', 'Trigram', 'Linear Interpolation'], index=['Vanilla', 'Laplace', 'UNK'])\n",
    "\n",
    "max_value = np.nanmax(perp_table_df[perp_table_df != np.inf])\n",
    "perp_table_df.replace([np.inf, -np.inf], max_value, inplace=True)\n",
    "\n",
    "perp_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f469b5-39df-4eaf-b115-fdff3c765b25",
   "metadata": {},
   "source": [
    "### g) Sentence Generation and Calculating Sentence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d43428f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_lm():\n",
    "    print(\"1. Vanilla LM\")\n",
    "    print(\"2. Laplace LM\")\n",
    "    print(\"3. UNK LM\")\n",
    "    lm_choice = input(\"Please choose a LM option from the above: \")\n",
    "\n",
    "    return lm_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "737dff73-204e-4ba7-9db3-03e44c8e1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enter_phrase():\n",
    "    # Get the user's input phrase\n",
    "    phrase = input(\"Enter a phrase: \")\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57e6d8ec-e687-4472-8305-4766443cdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(phrase):\n",
    "    # Initialize the vocabulary\n",
    "    vocabulary = set()\n",
    "\n",
    "    # Build the vocabulary from the input phrase\n",
    "    for word in phrase.strip().split():\n",
    "        vocabulary.add(word)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac5fcf11-83ff-4c06-a922-829406148652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, start_ngram, vocab, max_length=100):\n",
    "    sentence = list(start_ngram.split())\n",
    "    while True:\n",
    "        next_ngram = sentence[-2:]\n",
    "        # Check if the next n-gram is empty\n",
    "        if not next_ngram:\n",
    "            break\n",
    "        probs = compute_next_word_probs(model, next_ngram, vocab)\n",
    "        if not probs:\n",
    "            break\n",
    "        next_word = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "        sentence.append(next_word)\n",
    "        # Check if the sentence has reached the maximum length\n",
    "        if len(sentence) >= max_length:\n",
    "            break\n",
    "    return sentence + ['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44896b67-95f1-455c-8635-05cd38c45c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_new(model, vocab, start_phrase, num_words=50):\n",
    "    # Tokenize the starting phrase\n",
    "    tokens = start_phrase.strip().split()\n",
    "\n",
    "    # Generate the rest of the sentence\n",
    "    for _ in range(num_words):\n",
    "        # Get the previous n-gram\n",
    "        previous_ngram = tuple(tokens[-2:])\n",
    "\n",
    "        # Compute the probabilities of the next word\n",
    "        probs = compute_next_word_probs(model, previous_ngram, vocab)\n",
    "\n",
    "        # Sample the next word from the distribution\n",
    "        next_word = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "\n",
    "        # Add the next word to the tokens\n",
    "        tokens.append(next_word)\n",
    "\n",
    "        # Check if we have reached the end of the sentence\n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "\n",
    "    # Join the tokens into a sentence\n",
    "    sentence = ' '.join(tokens[:-1])\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97d62108-d393-4983-98a9-62391d0aface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_word_probs(freq_counts, prev_ngram, vocab):\n",
    "    # Check if freq_counts is None\n",
    "    if freq_counts is None:\n",
    "        return {}\n",
    "    if not prev_ngram:\n",
    "        return {}\n",
    "    if len(prev_ngram) == 1:\n",
    "        prev_word = prev_ngram[0]\n",
    "    else:\n",
    "        prev_word = prev_ngram[-1]\n",
    "    bigram = prev_ngram[0], prev_word\n",
    "    trigrams = {tuple(prev_ngram) + (word,) for word in freq_counts if word not in prev_ngram}\n",
    "    total_count = sum(freq_counts.values())\n",
    "    probs = {word: freq_counts.get(word,0) / total_count for word in freq_counts}\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        total_count = sum(freq_counts.get(word,0) for word in trigram)\n",
    "        probs[trigram[-1]] += freq_counts.get(trigram[-1],0) / total_count\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95c4ca39-c3e7-4889-a7ea-561ca4f941c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_probability(freq_counts, sentence, vocab):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sentence.strip().split() + ['</s>']\n",
    "\n",
    "    # Initialize the probability\n",
    "    prob = 1.0\n",
    "\n",
    "    # Iterate over each token in the sentence\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the current and previous n-grams\n",
    "        current_ngram = tuple(tokens[i:i+2])\n",
    "        previous_ngram = tokens[i-1:i+1] if i > 0 else ('<s>', tokens[i])\n",
    "\n",
    "        # Compute the probability of the current word given the previous n-gram\n",
    "        probs = compute_next_word_probs(freq_counts, previous_ngram, vocab)\n",
    "        prob *= probs.get(tokens[i], 0.0)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905cef9-8a5b-46a5-8173-2030907ca149",
   "metadata": {},
   "source": [
    "### Calculating the Probability of a Test Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6f4428c1-7f9c-4874-bb74-def794323309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_extracted_sentences[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c0f614c-5bf7-462b-b48d-0b5ef3cc221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.833827127912008e-70"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_probability = sen_probability(train_vanilla_dict_sen, sentence, get_vocabulary(sentence))\n",
    "\n",
    "vanilla_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "20ba7790-ede9-45db-9c25-531f3a61ca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7449306125222067e-70"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplace_probability = sen_probability(train_laplace_dict_sen, sentence, get_vocabulary(sentence))\n",
    "\n",
    "laplace_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40cd7488-f98d-450c-8aeb-cbc0f4e094a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.833827127912008e-70"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_probability = sen_probability(train_unk_dict_sen, sentence, get_vocabulary(sentence))\n",
    "\n",
    "unk_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268718bc-dfbc-4966-b382-07de005b6381",
   "metadata": {},
   "source": [
    "As expected, the sentence probability calculated using the vanilla and laplace smoothing models is very low, especially because a sentence from the test corpus is used. However, when using the UNK model, the probability is extremely high, possibly due to the high count of the < UNK > tags found in the model dictionaries. This is very similar to the calculaiton of linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef63f457-1fec-4fa9-9863-df07f9deb3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Vanilla LM\n",
      "2. Laplace LM\n",
      "3. UNK LM\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please choose a LM option from the above:  2\n",
      "Enter a phrase:  <s> i love to eat\n"
     ]
    }
   ],
   "source": [
    "lm_num = select_lm()\n",
    "model = None\n",
    "\n",
    "if lm_num == 1:\n",
    "    model = train_vanilla_dict_sen\n",
    "elif lm_num == 2:\n",
    "    model = train_laplace_dict_sen\n",
    "elif lm_num == 3:\n",
    "    model = train_unk_dict_sen\n",
    "\n",
    "inp_phrase = enter_phrase()\n",
    "\n",
    "phrase_vocabulary = get_vocabulary(inp_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ef7e996-a767-42c0-98a9-4fc80c316d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'i', 'love', 'to', 'eat', '</s>']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sentence = generate_sentence(model, inp_phrase, phrase_vocabulary)\n",
    "\n",
    "gen_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42cb77e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_prob = sen_probability(model, inp_phrase, phrase_vocabulary)\n",
    "\n",
    "sequence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820e128-38a9-4df9-badd-50621b9dbf1b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f0b44-2e07-4ccf-a6ce-1a9f0112998c",
   "metadata": {},
   "source": [
    "### 1. Corpus Text Extraction\n",
    "This part was tested using two checks, one for getting all the corpus files and another for getting the appropriate text. The first check was done by getting the file names of all the files in all the subdirectories of the main directory, putting them in a list, one for each category, and then printing them out. The second one was made by executing the extract text function on one of the files and then printing the returned text. The result of this was a list of the sentences of that file, including the < s > tags. Initially, all the sentences were put into as one whole element in the list, instead of a 2D list. This was because the words, which were contained inside the < w > tags, became grouped together. I fixed this by getting the whole text and separating the < s > tags, without including any unnecessary tags but not removing any content inside them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f83436-a562-4c0d-8343-4930a814aecd",
   "metadata": {},
   "source": [
    "### 2. Building Frequency Counts\n",
    "This part was tested by simpling printing the dictionaries generated by the build_frequency_counts function. Although the frequency counts were correctly returned, when the printing of the trigram frequency counts was attempted, it would always be unsuccessful due to the data rate limit. This was fixed by simply printing a portion of the dictionary. The same thing was done for the Laplace Smoothing frequency counts. This was not needed for the UNK frequency counts due to the decreased number of items in the trigram dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae55141-5a0b-45a3-b30e-1df1d9dcfc36",
   "metadata": {},
   "source": [
    "### 3. Linear Interpolation + Perplexity\n",
    "These two parts were tested by executing the linear_interpolation and calculate_perplexity functions on each of the different models on a test sentence. The tests ended up being successful. However, when these were implemented to create the perplexity table, there was one signficant problem that was encountered, which mainly took place on the n-gram probability functions. The issue was that the second parameter of the get function of the n-gram dictionaries was set too low to the point where the total probability would be exactly 0 after a number of executions. This was fixed by checking if the probability in the calculate_perplexity and fill_table functions is greater than 0 and setting the average nll to 100 and the interpolated probability to 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3546f-1172-4fbf-a27e-2cf8cbe2bc8b",
   "metadata": {},
   "source": [
    "### 4. Sentence Probability + Sentence Generation\n",
    "The sentence probability part was tested by taking a few sentences from the test corpus and calculating their probability using the unigram models, as it was the only way the task could be done. This was successful on the vanilla and laplace models. However, it was not the case for the UNK model for some sentences. In fact, when attempting to calculate the probability for these sentences, it will give an errr stating that a word could not be found. Various solutions were attempted, like trying to get the count from the < UNK > tag, but unfortunately I could not get it to work.\n",
    "\n",
    "The sentence generation was tested by inputting a phrase and attempting to generate a sentence. it was made sure that the input phrase begins with the < s > tag, to mark the start of the sentence. The sentence probability is also calculated. However, this did not work as well as intended, as instead of finding the next words to add to the input phrase, it just adds the < /s > tag. The function which ended up being used is called: generate_sentence(). Multiple solutions were intended, but unfortunately I could not get this to work either, as these generated lots of errors. The best attempted solution can be found in the function called: generate_sentence_new(). Despite this, the sentence probability worked without any errors, although the result was always 0, which was not intended. This could not be solved either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a032c96-9e25-4125-92f4-02c0c217ba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Matthias Vassallo Pulis"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "title": "ICS2203 - Language Model Assignment"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
